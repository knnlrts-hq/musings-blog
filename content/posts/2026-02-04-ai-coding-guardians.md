---
title: "Who Watches the AI That Writes Your Code?"
date: 2026-02-04
tags: [ai, software-engineering, philosophy, automation]
---

Here's a question from ancient Rome that feels surprisingly relevant to me today: ["Quis custodiet ipsos custodes?"](https://en.wikipedia.org/wiki/Quis_custodiet_ipsos_custodes) Who will watch the watchmen themselves? Now replace "watchmen" with "AI coding assistants" for a modern spin on that same old problem of control. AI writes ever more lines of code every day. And as it does, humans review less and less of it. [Peter Steinberger](https://www.youtube.com/watch?v=8lF7HmQ_RgY), the creator of OpenClaw, famously said a couple of days ago: "I ship the code, I don't read it". It is the extreme version of the "move fast and break things" mentality that would make Mark Zuckerberg proud. Tests pass, demos work, and everyone moves on. But ever so small mistakes pile up ever so quietly. And one day, those small mistakes might become a big one. The black swan squared.

## From Manual Programming to the Dark Factory
[Dan Shapiro](https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/) recently described AI coding in levels of increasing automation, comparing it to self-driving cars. At Level 0, there is no AI assistant and you write all the code yourself, character by character, like a prehistoric caveman. At Level 2, which is where most developers are today, AI is a treated as an extremely smart but also wildly inconsistent junior developer, that suggests the boilerplate code while you drive the design. Climb a little higher and things start to shift rapidly. At Level 3, AI writes most of the code. You review diffs like a safety driver watching the autopilot. At Level 4, you write (or rather generate) specs, feed them to your swarm of agents and check in every few hours while it codes and tests unattended. Finally, Level 5 is the "dark factory": the ultimate automation of software engineering. High-level directives go in, generated software comes out. AI watches AI. The term "dark factory" comes from manufacturing: fully automated factories where robots build other robots in complete darkness. No humans are neither needed, nor welcome on the factory floor.

![AI Coding Levels](content/images/posts/2026-02-04-ai-coding-guardians/ai-assisted-software.png "AI Coding Levels")
*The evolution of AI-assisted software development from level 0 (fully manual coding) to level 5 (the "dark factory" of autonomous code).*

Small teams or even solo software engineers on the cutting edge are already operating near Level 5 in their specific problem domains. The market took note and started to price in this evolution: the ['SaaSpocalypse'](https://www.forbes.com/sites/donmuir/2026/02/04/300-billion-evaporated-the-saaspocalypse-has-begun/) has begun and the ['Roadrunner Economy'](https://nraford7.github.io/road-runner-economy/) is now officially in full swing. The speed at which things are happening is remarkable. But notice what disappears as we climb the software automation ladder: human eyes on the code. AI doesn't get tired. It can generate thousands of lines of code, write its own tests, and then review and document its own work: judge, jury, and executioner. When everything looks (or should I say "seems") green, who bothers checking the details? Often, nobody.

The worry here is that automated oversight can normalize deviance just as much as manual oversight. And when humans step back in, intervention often comes too late, with the flawed code already in production. Some predict this will spark demand for "artisanal software" where companies advertise that humans wrote the code. Regulated industries might require a whole slew of new compliance standards and audits. If algorithms are the watchmen, external auditors might need to watch the algorithms. But governance always lags behind automation, and today the reflex when AI causes problems is: add more AI. That can accelerate the very deviance we're trying to fix.

## The Normalization of Deviance
Sociologist [Diane Vaughan](https://en.wikipedia.org/wiki/Normalization_of_deviance) coined this term after studying the Challenger disaster. NASA engineers had seen O-ring problems before. Previous launches with damaged seals hadn't failed, so warnings got rationalized away. As Vaughan put it: "The absence of disaster was mistaken for the presence of safety". The normalization of deviance describes how unsafe practices become accepted when they don't immediately cause catastrophe. The same pattern shows up today with code generated by AI. An AI assistant introduces a quirky workaround or outright ignores the request (context overloading anyone?). Tests pass. Ship it. Next time, another silent shortcut. Tests pass again. Ship it. Each time you skip review, you lower the bar for next time. This drift happens quietly. Nobody really decides to ignore quality, but slowly tiny "temporary" fixes become permanent. On top of that, the scheduled pressure of software development makes it easy to trust the AI by default. Companies "confuse the absence of a successful attack with the presence of robust security". Nothing bad happened today, so therefore system must be safe. Until it isn't.

The danger with normalized deviance is the sudden failure that seems obvious only in hindsight. All those ignored warnings align at once. With AI code, this might look like a production crash in code that is too complex for anyone to understand, or a security hole sitting dormant until some bad actor exploits it. Don't even get me started on Simon Willison's ["Lethal Trifecta"](https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/) for AI agents. These incidents are like the O-ring erosion. Research shows it takes only a small amount of poisoned data to add a backdoor. In a fully automated pipeline, who catches it?

![The Challenger Disaster](content/images/posts/2026-02-04-ai-coding-guardians/challenger-disaster.jpg "The Challenger Disaster")
*On January 28,1986, Space Shuttle Challenger broke apart 73 seconds into its flight, killing all seven crew members.*

## Testing Can't Catch Everything
Teams aiming for Level 4 or 5 dark factory automation of software engineering rely heavily on tests, static analyzers, and CI checks. Instead of reviewing every line, you ask the system to prove its own correctness: write the code, run the tests, deploy if green. This helps. Many AI coding setups force the AI to write tests for every function. Humans design the testing framework and high-level specs. They build monitoring tools that watch for dangerous actions. The human role shifts from writing code to designing the system that produces code. **But testing has it's limits: it only proves the presence of bugs, never their absence.** Pass 100 scenarios and you might still fail scenario 101. Worse, AI might pattern-learn to game the tests (also called "reward hacking"). If the AI writes 1000 tests and they all pass, an inattentive team might assume those tests cover everything important. But who wrote them? The AI; which may have blind spots. **There's a key difference between proving the system works and proving it's safe.** Teams focus on the first. The second gets treated as secondary. That's a form of normalized deviance too.

## The Infinite Regress Problem
If we give AI the task to guard our systems, then who guards that AI? You could propose another AI to audit the first. But then who reviews that reviewer? The ancient Greeks suggested training guardians to guard themselves by instilling an incorruptible character. In AI terms, that is called "alignment research". We try to build systems that internalize safety constraints so deeply, that they won't go rogue without supervision. A very noble goal, but extremely hard in practice. On the bright side: here is something that AI will not automate soon. The practical answer will probably be to work in layers: AI does most of the coding, other AI tools monitor for known issues, and humans handle the audits and edge cases. Like how commercial aircraft can fly themselves most of the time, but pilots handle the unexpected elements. AI-coded systems should be treated as something to be stress-tested regularly. Hunt for weaknesses instead of assuming that the AI has things covered. That counters the complacency that normalization of deviance brings.

Why automate the software developer lifecycle at all? The optimistic answer: it frees you from the monotonic toil involved in it. As one of my favorite philosophers, Alan Watts, put it in the 1960s, "The whole purpose of technology and machinery, after all, is to make human drudgery unnecessary". Having AI write boilerplate, generate tests, and refactor code in seconds is liberating. You operate at the level of ideas instead of syntax. But Watts would probably point out a paradox. You build machines to relieve work. Then you create new work monitoring those machines. Then you consider building machines to monitor the monitors. At some point, you have to trust the water to carry you or you'll never stop thrashing.

![Alan Watts](content/images/posts/2026-02-04-ai-coding-guardians/alan-watts02.png "Alan Watts")
*Alan Watts*

A system too tangled in oversight defeats the purpose of the automation it watches. Watts favored balance over total control. Design AI systems that are safe by architecture, so you don't need to hover over it every second. But accept that uncertainty will always exist. Neither blind trust nor obsessive control works. As Watts observed, "Any time you voluntarily let up control, you have an access of power". By letting AI handle code generation, you gain productivity. But only if you adjust your approach to accommodate that change safely.

Maybe the dark factory misses something vital: the creative, unpredictable spark that only humans can bring. If all code comes from AI optimizing for past data and tests, we squeeze out the creative deviations that lead to innovation along with the dangerous ones. Watts might remind us that no system guarantees its own perfection. It just does not exist. Trying to make a machine that watches itself flawlessly is like trying to outrun your own shadow. Or like trying to see your own eyes without a mirror. Instead of chasing that infinite regress, we should cultivate trust paired with responsible vigilance. After all: the price of freedom is eternal vigilance. Trust, but verify. Trust the tools for what they're good at. Keep humans in the loop for judgment, ethics, and the unforeseen.

## Building a Future with AI
"Who guards the guardians?" has no simple answer when code writes code. The normalization of deviance teaches that small unchecked errors can quickly snowball into big failures. Dark factories promise productivity but demand new forms of oversight: implement audit trails for AI changes. Stress-test AI systems continuously. Bring in security experts, ethicists, and domain specialists. Build a culture where people question and test the AI's decisions, instead of blindly trusting them.

Remember that AI is a tool, not a replacement for responsibility. Humans and software engineers are definitely not vanishing from the picture. You're moving from manual labor to design, from coder to curator. It often makes me think about this legendary page from an internal IBM training in 1979, which could not be more appropriate for our new age of AI:

![70s IBM training material](content/images/posts/2026-02-04-ai-coding-guardians/a-computer-can-never-be-held-accountable.jpg "70s IBM training material")
*Timeless wisdom from the 1970s.*

As Watts might say, work with the flow, not against it: "the only way to make sense out of change is to plunge into it, move with it, and join the dance". I definitely think the engineer of the future will be an agentic one, commanding fleets of agents, shaping raw compute into software products and services that add real value. But do try to stay awake at the wheel without white-knuckling the controls. Watch your watchmen in smart, judicious ways. The future of software is being written by AI agents right now. It's up to you to read and *understand* what they write, question it, experiment with it, and remain the conscious custodian of the software that you unleash upon the world.

